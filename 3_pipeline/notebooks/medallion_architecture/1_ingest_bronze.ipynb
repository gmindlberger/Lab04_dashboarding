{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "120a4ec7-7e2c-4dbd-82d0-ac31c3dcd8e9",
   "metadata": {},
   "source": [
    "# Bronze: Ingest data\n",
    "The bronze lazer of the [medallion architecture](https://www.databricks.com/glossary/medallion-architecture) is mainly responsible to ingest data from different sources.\n",
    "The logic below reads \"streaming\" data via a Kafka Topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89548f98-a203-43fc-a6c2-261af5f65d02",
   "metadata": {},
   "source": [
    "## Query the API and publish to the topic\n",
    "Start the **weather_producer** to query the open API: open-meteo.com to generate new messages for the given Kafka topic.\n",
    "\n",
    "**NOTE**: Ensure the topic **weather-data-pipeline** is available!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf23b2d7-381d-4b38-8929-36f0c0a4fb1c",
   "metadata": {},
   "source": [
    "## Consume from Kafka topic\n",
    "The approch is to \"listen\" to data in the topic and store the data in [parquet](https://parquet.apache.org/) files on the storage layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb8dadfd-ae0d-466b-b709-5efeb9b27f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-30 17:12:44.282315] Stored 96 records to MinIO at: bronze/2025-05-30/weather_1748625164.205021.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "import datetime\n",
    "\n",
    "# Kafka & MinIO definitions\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "KAFKA_BROKER = \"kafka:9092\"\n",
    "TOPIC = \"weather-data-pipeline\"\n",
    "\n",
    "MINIO_ENDPOINT = \"http://minio:9000\"\n",
    "MINIO_ACCESS_KEY = \"admin\"\n",
    "MINIO_SECRET_KEY = \"password\"\n",
    "BUCKET_NAME = \"weather-data\"\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaWeatherToMinIOOptimized\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize MinIO client\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY\n",
    ")\n",
    "\n",
    "# Create bucket if it doesn't exist\n",
    "def init_bucket(bucket_name):\n",
    "    buckets = [b['Name'] for b in s3_client.list_buckets().get('Buckets', [])]\n",
    "    if bucket_name not in buckets:\n",
    "        s3_client.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "init_bucket(BUCKET_NAME)\n",
    "\n",
    "# Define schema for Kafka 'value' field\n",
    "weather_schema = StructType() \\\n",
    "    .add(\"timestamp\", StringType()) \\\n",
    "    .add(\"temperature\", DoubleType()) \\\n",
    "    .add(\"humidity\", DoubleType()) \\\n",
    "    .add(\"wind_speed\", DoubleType())\n",
    "\n",
    "# Read Stream from Kafka\n",
    "df_raw = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Deserialize JSON from Kafka 'value'\n",
    "df_parsed = df_raw.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(col(\"json_str\"), weather_schema).alias(\"data\")) \\\n",
    "    .select(\n",
    "        col(\"data.timestamp\"),\n",
    "        col(\"data.temperature\"),\n",
    "        col(\"data.humidity\"),\n",
    "        col(\"data.wind_speed\")\n",
    "    )\n",
    "\n",
    "# Write to MinIO using foreachBatch\n",
    "def write_to_minio(batch_df, batch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "\n",
    "    batch_df = batch_df.withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "    pdf = batch_df.toPandas()\n",
    "\n",
    "    now = datetime.datetime.now(datetime.timezone.utc)\n",
    "    current_date = now.strftime(\"%Y-%m-%d\")\n",
    "    parquet_key = f\"bronze/{current_date}/weather_{now.timestamp()}.parquet\"\n",
    "\n",
    "    buffer = BytesIO()\n",
    "    pdf.to_parquet(buffer, engine=\"pyarrow\", index=False)\n",
    "    s3_client.put_object(Bucket=BUCKET_NAME, Key=parquet_key, Body=buffer.getvalue())\n",
    "\n",
    "    print(f\"[{datetime.datetime.now()}] Stored {len(pdf)} records to MinIO at: {parquet_key}\")\n",
    "\n",
    "# Start the streaming query with 5 seconds trigger\n",
    "query = df_parsed.writeStream \\\n",
    "    .foreachBatch(write_to_minio) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4693e5c7-680a-4e29-bf46-a80d39c91129",
   "metadata": {},
   "source": [
    "## Shutdown the query\n",
    "You can have a look and view the monitoring of spark jobs with the Spark UI: [http://localhost:4040](http://localhost:4040)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cca11f9-44a4-488f-a0e6-4b483bdf1bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is active: True\n",
      "Is active: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Is active:\", query.isActive)\n",
    "query.stop()\n",
    "print(\"Is active:\", query.isActive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00a16e-bb4a-47fa-8f5c-e101a9b41739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
