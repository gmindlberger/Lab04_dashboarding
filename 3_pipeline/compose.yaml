services:
  # jupyter notebook with pyspark integration
  jupyter-spark:
    # a custom container-image is built to add necessary packages
    build:
      context: ./src/container
      dockerfile: jupyter_pyspark.Dockerfile
    container_name: jupyter-spark
    hostname: jupyter-spark
    environment:
      - JUPYTER_TOKEN=easy
    volumes:
      # if you are on linux with selinux (most distros these days)
      # you need to take care the volume is mounted with the defined selinux labels
      # https://docs.docker.com/engine/storage/bind-mounts/#configure-the-selinux-label
      - ./notebooks:/home/jovyan/notebooks:z
      - ./input_data:/home/jovyan/notebooks/input_data:z
    working_dir: /home/jovyan/notebooks
    networks:
      - lab-network
    ports:
      - "8888:8888" # notebook UI
      - "4040:4040" # UI for spark-jobs
    
  # https://min.io/
  # s3 compatible storage provided by minio
  minio:
    image: minio/minio:latest
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
    networks:
      - lab-network
    ports:
      - "9000:9000"   # MinIO API
      - "9001:9001"   # MinIO Console
    volumes:
      - minio_data:/data

  # https://kafka.apache.org/
  # Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance 
  # data pipelines, streaming analytics, data integration, and mission-critical applications.
  kafka:
    image: bitnami/kafka:4.0
    container_name: kafka
    hostname: kafka
    environment:
       # KRaft settings
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      # Listeners
      # SRC: https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/
      # this is rather tricky to enable communication within the docker-network (compose-setup)
      # but also enable connectivity from the "outside" of docker
      # the solution is to configure 2 listeners
      #  - INTERNAL: used for intra-docker-network communication, where the hostname kafka ca be resolved
      #  - EXTERNAL: used to connect to kafka from the outside world (outside in the sense of not docker)
      - KAFKA_CFG_LISTENERS=INTERNAL://:9092,CONTROLLER://:9093,EXTERNAL://:19092
      # to make this happen we use one additional listener and tell kafka to advertise the second listener with 
      # localhost:19092 --> this port is mapped and can be accessed from the outside
      - KAFKA_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://localhost:19092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL
    networks:
      - lab-network
    ports:
      - "9092:9092"
      - "19092:19092"

networks:
  lab-network:
    name: lab-network

volumes:
  minio_data:
    driver: local